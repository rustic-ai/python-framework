#!/usr/bin/env python
"""
Simple LLM Agent Example

This example demonstrates how to create an agent that integrates with external LLM services.
It shows two approaches:
1. Direct API calls (OpenAI)
2. Using dependency injection (mock LLM service for demo)

Run this example with:
    python examples/demo_projects/simple_llm_agent.py
"""

import asyncio
import time
import os
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field

from rustic_ai.core.guild import Agent, agent
from rustic_ai.core.guild.builders import AgentBuilder, GuildBuilder
from rustic_ai.core.guild.dsl import AgentSpec, BaseAgentProps, DependencySpec
from rustic_ai.core.guild.agent_ext.depends.dependency_resolver import DependencyResolver
from rustic_ai.core.agents.testutils.probe_agent import ProbeAgent


# ==== Message Models =====

class LLMRequest(BaseModel):
    """A request to generate text using an LLM."""
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.7
    system_prompt: Optional[str] = Field(default="You are a helpful assistant.")


class LLMResponse(BaseModel):
    """A response from an LLM."""
    generated_text: str
    model_used: str
    processing_time: float


# ==== Mock LLM Service =====

class LLMService:
    """A simple mock LLM service."""
    
    def __init__(self, model: str = "gpt-3.5-turbo", api_key: Optional[str] = None):
        self.model = model
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY", "mock_api_key")
        print(f"LLMService initialized with model: {model}")
    
    def generate_text(self, prompt: str, system_prompt: str, max_tokens: int = 100, temperature: float = 0.7) -> Dict[str, Any]:
        """Generate text using the LLM."""
        print(f"LLM: Generating text for prompt: '{prompt[:50]}...' (if longer)")
        
        # Sleep to simulate API call
        time.sleep(1.2)
        
        # For a real implementation, you would call the actual API
        # Here we're just returning mock data
        
        # Mock implementation for demo purposes
        generated_text = f"This is a response to: '{prompt}'. Generated by a mock LLM service using model {self.model}."
        
        # Change to use real API here if desired:
        # import openai
        # openai.api_key = self.api_key
        # response = openai.Completion.create(
        #     model=self.model,
        #     prompt=prompt,
        #     max_tokens=max_tokens,
        #     temperature=temperature,
        # )
        # generated_text = response.choices[0].text
        
        return {
            "text": generated_text,
            "model": self.model,
            "usage": {"completion_tokens": 50, "prompt_tokens": len(prompt) // 4, "total_tokens": 50 + len(prompt) // 4}
        }


# ==== Dependency Resolver =====

class LLMServiceResolver(DependencyResolver):
    """Resolver for the LLMService dependency."""
    
    def __init__(self, model: str = "gpt-3.5-turbo", api_key: str = None):
        self.model = model
        self.api_key = api_key
        self._llm_instance = None
    
    def resolve(self, guild_id: str, agent_id: str = None) -> LLMService:
        """Resolve the LLM service."""
        if self._llm_instance is None:
            self._llm_instance = LLMService(self.model, self.api_key)
        return self._llm_instance


# ==== Agent Implementation with Dependency Injection =====

class LLMAgentProps(BaseAgentProps):
    """Properties for the LLM agent."""
    default_model: str = "gpt-3.5-turbo"
    default_max_tokens: int = 150
    default_temperature: float = 0.7
    default_system_prompt: str = "You are a helpful assistant."


class LLMAgent(Agent[LLMAgentProps]):
    """
    An agent that generates text using an LLM service.
    
    This agent demonstrates dependency injection for the LLM service.
    """
    
    def __init__(self, agent_spec: AgentSpec[LLMAgentProps]):
        super().__init__(agent_spec)
        # Get defaults from props
        self.default_model = agent_spec.props.default_model
        self.default_max_tokens = agent_spec.props.default_max_tokens
        self.default_temperature = agent_spec.props.default_temperature
        self.default_system_prompt = agent_spec.props.default_system_prompt
        print(f"LLMAgent initialized with ID: {self.id}")
    
    @agent.processor(clz=LLMRequest, depends_on=["llm_service"])
    def generate_text(self, ctx: agent.ProcessContext[LLMRequest], llm_service: LLMService):
        """Generate text using the injected LLM service."""
        request = ctx.payload
        
        print(f"[{self.name}] Generating text for prompt: '{request.prompt[:50]}...' (if longer)")
        
        # Use request params or fall back to defaults
        max_tokens = request.max_tokens or self.default_max_tokens
        temperature = request.temperature or self.default_temperature
        system_prompt = request.system_prompt or self.default_system_prompt
        
        # Record start time for performance tracking
        start_time = time.time()
        
        # Use the injected LLM service
        result = llm_service.generate_text(
            prompt=request.prompt,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        # Send the response
        ctx.send(LLMResponse(
            generated_text=result["text"],
            model_used=result["model"],
            processing_time=processing_time
        ))
        
        print(f"[{self.name}] Text generated in {processing_time:.2f} seconds")


# ==== Direct API Agent Implementation =====

class DirectLLMAgent(Agent[LLMAgentProps]):
    """
    An agent that generates text by directly calling an LLM service.
    
    This agent demonstrates direct API integration without dependency injection.
    """
    
    def __init__(self, agent_spec: AgentSpec[LLMAgentProps]):
        super().__init__(agent_spec)
        # Get defaults from props
        self.default_model = agent_spec.props.default_model
        self.default_max_tokens = agent_spec.props.default_max_tokens
        self.default_temperature = agent_spec.props.default_temperature
        self.default_system_prompt = agent_spec.props.default_system_prompt
        
        # Create the LLM service directly
        self.llm_service = LLMService(model=self.default_model)
        print(f"DirectLLMAgent initialized with ID: {self.id}")
    
    @agent.processor(clz=LLMRequest)
    def generate_text(self, ctx: agent.ProcessContext[LLMRequest]):
        """Generate text by directly calling the LLM service."""
        request = ctx.payload
        
        print(f"[{self.name}] Generating text for prompt: '{request.prompt[:50]}...' (if longer)")
        
        # Use request params or fall back to defaults
        max_tokens = request.max_tokens or self.default_max_tokens
        temperature = request.temperature or self.default_temperature
        system_prompt = request.system_prompt or self.default_system_prompt
        
        # Record start time for performance tracking
        start_time = time.time()
        
        # Use the direct LLM service
        result = self.llm_service.generate_text(
            prompt=request.prompt,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        # Send the response
        ctx.send(LLMResponse(
            generated_text=result["text"],
            model_used=result["model"],
            processing_time=processing_time
        ))
        
        print(f"[{self.name}] Text generated in {processing_time:.2f} seconds")


async def main():
    # Create and launch a guild
    guild = GuildBuilder("llm_guild", "LLM Guild", "A guild with LLM agents") \
        .launch(add_probe=True)
    
    # Get the probe agent for monitoring messages
    probe_agent = guild.get_agent_of_type(ProbeAgent)
    print(f"Created guild with ID: {guild.id}")
    
    # Define the LLM service dependency
    dependencies = {
        "llm_service": DependencySpec(
            class_name="__main__.LLMServiceResolver",
            properties={"model": "gpt-4", "api_key": "your-api-key-here"}
        )
    }
    
    # Create and launch the dependency injection agent
    llm_agent_props = LLMAgentProps(
        default_model="gpt-4",
        default_max_tokens=200,
        default_temperature=0.8,
        default_system_prompt="You are a knowledgeable assistant specialized in Python programming."
    )
    
    di_agent_spec = AgentBuilder(LLMAgent) \
        .set_name("LLM-DI") \
        .set_description("LLM agent using dependency injection") \
        .set_properties(llm_agent_props) \
        .set_dependency_map(dependencies) \
        .build_spec()
    
    # Create and launch the direct API agent
    direct_agent_spec = AgentBuilder(DirectLLMAgent) \
        .set_name("LLM-Direct") \
        .set_description("LLM agent using direct API calls") \
        .set_properties(llm_agent_props) \
        .build_spec()
    
    guild.launch_agent(di_agent_spec)
    guild.launch_agent(direct_agent_spec)
    
    print("\nAgents in the guild:")
    for agent_spec in guild.list_agents():
        print(f"- {agent_spec.name} (ID: {agent_spec.id}, Type: {agent_spec.class_name})")
    
    # Test the dependency injection agent
    print("\n--- Testing LLM Agent with Dependency Injection ---")
    probe_agent.publish("default_topic", LLMRequest(
        prompt="Explain what is dependency injection in software development?",
        max_tokens=150,
        temperature=0.7
    ))
    await asyncio.sleep(1.5)  # Wait for processing
    
    # Get and print messages
    messages = probe_agent.get_messages()
    for msg in messages:
        if "generated_text" in msg.payload:
            print(f"\nResponse from {msg.sender.name}:")
            print(f"Model: {msg.payload['model_used']}")
            print(f"Processing time: {msg.payload['processing_time']:.2f}s")
            print(f"Generated text: {msg.payload['generated_text']}")
    
    # Clear messages
    probe_agent.clear_messages()
    
    # Test the direct API agent
    print("\n--- Testing LLM Agent with Direct API ---")
    probe_agent.publish("default_topic", LLMRequest(
        prompt="What are the key principles of clean code?",
        max_tokens=150,
        temperature=0.7
    ))
    await asyncio.sleep(1.5)  # Wait for processing
    
    # Get and print messages
    messages = probe_agent.get_messages()
    for msg in messages:
        if "generated_text" in msg.payload:
            print(f"\nResponse from {msg.sender.name}:")
            print(f"Model: {msg.payload['model_used']}")
            print(f"Processing time: {msg.payload['processing_time']:.2f}s")
            print(f"Generated text: {msg.payload['generated_text']}")
    
    # Shutdown the guild
    guild.shutdown()
    print("\nGuild shutdown complete")


if __name__ == "__main__":
    asyncio.run(main()) 